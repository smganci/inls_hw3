{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INLS Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarah Ganci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11/24/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn import preprocessing \n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotators = pd.read_csv(\"hw3_data/hw3_data/music.train.annotators1.csv\", encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>a4</th>\n",
       "      <th>a5</th>\n",
       "      <th>a6</th>\n",
       "      <th>a7</th>\n",
       "      <th>a8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oh! boy! how things have change..years ago i ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        a1        a2  \\\n",
       "0   oh! boy! how things have change..years ago i ...  positive  positive   \n",
       "\n",
       "         a3        a4        a5        a6        a7        a8  \n",
       "0  positive  positive  positive  positive  positive  negative  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotators.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(997, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotators.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test=pd.read_csv(\"hw3_data/hw3_data/music.test.csv\",encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joan. you don't love me. because i'm a guy. b...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>so far so good...so what! (in it's origional ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     class\n",
       "0   joan. you don't love me. because i'm a guy. b...  positive\n",
       "1   so far so good...so what! (in it's origional ...  negative"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_y = ['negative','positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder() \n",
    "le.fit(class_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_y= le.transform(df_test['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods of Annotation Combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Score By Majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#name: majority_wins\n",
    "#in: row from datafram object (expects a list)\n",
    "#out: returns 1 if pos is >=neg, returns 0 if neg > pos\n",
    "def majority_wins(row):\n",
    "    neg= row[1:9].count(\"negative\")\n",
    "    pos=row[1:9].count(\"positive\")\n",
    "   # print(\"pos: \"+str(pos)+ \" neg: \"+str(neg))\n",
    "    return (0,1)[pos >= neg]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'negative']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test data\n",
    "df_annotators.as_matrix()[1].tolist()[1:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test function\n",
    "majority_wins(df_annotators.as_matrix()[1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def majority_wins_convert(df):\n",
    "    out=[]\n",
    "    matrix=df.as_matrix()\n",
    "    for i in range(len(matrix)):\n",
    "        out.append(majority_wins(matrix[i].tolist()))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method1_labels= majority_wins_convert(df_annotators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Instance Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#name: confidence\n",
    "#in: df row (expected list from df_annotators)\n",
    "#out: a tuple with 1 for pos 0 for neg and the score of how confident the score is\n",
    "def confidence(row):\n",
    "    neg= row[1:9].count(\"negative\")\n",
    "    pos=row[1:9].count(\"positive\")\n",
    "   # if neg< pos, then return 0 and the number by how much more neg than pos\n",
    "    #if pos>=neg, then return 1 and the number by how much more pos than neg\n",
    "    return ([0,neg-pos],[1,pos-neg])[pos >= neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence(df_annotators.as_matrix()[1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#in: dataframe (expected df_annotators)\n",
    "#out: new dataframe object with weighted rows (duplicates of rows) and labeled columns\n",
    "def confidence_convert(df):\n",
    "    #out x will hold the text data (with duplicates)\n",
    "    out_x=[]\n",
    "    \n",
    "    #out y will hold the labels for each text data (also duplicates)\n",
    "    out_y=[]\n",
    "    \n",
    "    #text_df stores the df text column for ease of adding\n",
    "    text=df['text']\n",
    "    \n",
    "    #matrix holds the df in matrix form to be passed\n",
    "    matrix=df.as_matrix()\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        con=confidence(matrix[i].tolist())\n",
    "        for j in range(con[1]):\n",
    "            out_x.append(matrix[i][0])\n",
    "            out_y.append(con[0])\n",
    "            \n",
    "    return [out_y,out_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "method2_labels, method2_x=confidence_convert(df_annotators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of labels: 3808\n",
      "size of x: 3808\n"
     ]
    }
   ],
   "source": [
    "print(\"size of labels: \"+ str(len(method2_labels)))\n",
    "print(\"size of x: \"+ str(len(method2_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Best Annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val(classifier, x, y, cv):\n",
    "    return np.mean(cross_val_score(classifier, x, y, cv=cv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validate Score for Each Annotator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the consistancy of each annotator by using their labels to make predictions.  We can use crossvalidation to calculate the cross validation score.  Annotators with higher scores will are more consistant in their ratings.  Hypothetically, if we use the more consistant annotators, then the overall accuracy of the model should be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make vectorizor\n",
    "tf_m3 = TfidfVectorizer(min_df=1,stop_words='english',max_features=2000)\n",
    "#extract features from train x\n",
    "x_tfidf_m3=tf_m3.fit_transform(df_annotators['text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotators=[]\n",
    "for i in range(1,9):\n",
    "    mnb1= MultinomialNB(alpha=1)\n",
    "    annotators.append([i, cross_val(mnb1, x_tfidf_m3, df_annotators[\"a\"+str(i)], 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 0.52251515151515149]\n",
      "[6, 0.53556565656565658]\n",
      "[5, 0.63298289012574727]\n",
      "[3, 0.70008260826082602]\n",
      "[1, 0.73621662166216617]\n",
      "[2, 0.76528882888288818]\n",
      "[7, 0.81554695469546945]\n",
      "[8, 0.85156277056277063]\n"
     ]
    }
   ],
   "source": [
    "sorted_annotators=sorted(annotators, key= lambda tup: tup[1])\n",
    "for i in range(len(sorted_annotators)):\n",
    "    print(sorted_annotators[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The annotators in order of their consistancy are: 8, 7, 2, 1, 3, 5, 6, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods now allow for selection of annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#name: confidence\n",
    "#in: df row (expected list from df_annotators), column numbers in an array\n",
    "#out: a tuple with 1 for pos 0 for neg and the score of how confident the score is\n",
    "def confidence_tators(row, tators):\n",
    "    scores=[]\n",
    "    for i in range(len(tators)):\n",
    "        scores.append(row[tators[i]])\n",
    "       # print(row[tators[i]])\n",
    "    neg= scores.count(\"negative\")\n",
    "    pos=scores.count(\"positive\")\n",
    "   # if neg< pos, then return 0 and the number by how much more neg than pos\n",
    "    #if pos>=neg, then return 1 and the number by how much more pos than neg\n",
    "    return ([0,neg-pos],[1,pos-neg])[pos >= neg]\n",
    "\n",
    "\n",
    "def majority_wins_tators_convert(df, tators):\n",
    "    out=[]\n",
    "    matrix=df.as_matrix()\n",
    "    for i in range(len(matrix)):\n",
    "        out.append(confidence_tators(matrix[i].tolist(),tators)[0])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#majority_wins_tators_convert(df_annotators, [8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test majority wins for all diff combos of top annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_combo=[]\n",
    "test_tators=[[8],[8,7], [8, 7, 2], [8, 7, 2, 1], [8, 7, 2, 1, 3], [8, 7, 2, 1, 3, 5], [8, 7, 2, 1, 3, 5, 6], [8, 7, 2, 1, 3, 5, 6, 4]]\n",
    "for i in range(len(test_tators)):\n",
    "    mnb1= MultinomialNB(alpha=1)\n",
    "   # print(test_tators[i])\n",
    "    y_labels=majority_wins_tators_convert(df_annotators, test_tators[i])\n",
    "   # print(y_labels)\n",
    "    cv_score=cross_val(mnb1, x_tfidf_m3, y_labels, 10)\n",
    "    annotator_combo.append([test_tators[i], cv_score])\n",
    "#print(annotator_combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 7, 2, 1, 3, 5, 6, 4], 0.73029828901257465]\n",
      "[[8, 7, 2, 1, 3, 5, 6], 0.75024242424242427]\n",
      "[[8, 7, 2, 1, 3], 0.75630818387961241]\n",
      "[[8, 7, 2], 0.76030313031303132]\n",
      "[[8, 7, 2, 1], 0.77134373437343728]\n",
      "[[8, 7, 2, 1, 3, 5], 0.77230853085308537]\n",
      "[[8, 7], 0.78637373737373739]\n",
      "[[8], 0.85156277056277063]\n"
     ]
    }
   ],
   "source": [
    "sorted_annotator_combo=sorted(annotator_combo, key= lambda tup: tup[1])\n",
    "for i in range(len(sorted_annotator_combo)):\n",
    "    print(sorted_annotator_combo[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best combination of annotators was just annotator 8 (the top 1 annotator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "method3_labels_top1= majority_wins_tators_convert(df_annotators, [8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method3_labels_top2=majority_wins_tators_convert(df_annotators, [8, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method3_labels_top3=majority_wins_tators_convert(df_annotators, [8, 7, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method3_labels_top4=majority_wins_tators_convert(df_annotators, [8, 7, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method3_labels_top5=majority_wins_tators_convert(df_annotators, [8, 7, 2, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method3_labels_top6=majority_wins_tators_convert(df_annotators, [8, 7, 2, 1, 3, 5 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method3_labels_top7=majority_wins_tators_convert(df_annotators, [8, 7, 2, 1, 3, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method3_labels_top8=majority_wins_tators_convert(df_annotators, [8, 7, 2, 1, 3, 5, 6, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#in: dataframe of x features, y labels, test_x tfidf vector, test_y labels\n",
    "#out: confusion matrix\n",
    "def model_predict_print(train_x, train_y, test_x, test_y):\n",
    "    #make model\n",
    "    mnb = MultinomialNB(alpha=1.0)\n",
    "    \n",
    "    #make vectorizor\n",
    "    tf = TfidfVectorizer(min_df=1,stop_words='english',max_features=2000)\n",
    "    \n",
    "    #extract features from train x\n",
    "    x_tfidf=tf.fit_transform(train_x).toarray()\n",
    "    \n",
    "    #fit mnb model based on the train x text and train y labels\n",
    "    mnb.fit(x_tfidf,train_y)\n",
    "    \n",
    "    #create x_test_tfidf by transforming test_x using tf\n",
    "    x_test_tfidf=tf.transform(test_x).toarray()\n",
    "    \n",
    "    #store predictions from x_test tfidf array in predictions\n",
    "    predictions=mnb.predict(x_test_tfidf)\n",
    "    \n",
    "    #calculate confusion matrix using method\n",
    "    print(\"Confusion Matrix:\")\n",
    "    \n",
    "    print(confusion_matrix(test_y, predictions ))\n",
    "    #calculate accuracy using accuracy_score method \n",
    "    \n",
    "    print(\"Accuracy: \")\n",
    "    print(accuracy_score(test_y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Score By Majority Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[360 121]\n",
      " [126 393]]\n",
      "Accuracy: \n",
      "0.753\n"
     ]
    }
   ],
   "source": [
    "model_predict_print(df_annotators['text'],method1_labels,df_test['text'],test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Instance Weighting Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[427  54]\n",
      " [247 272]]\n",
      "Accuracy: \n",
      "0.699\n"
     ]
    }
   ],
   "source": [
    "model_predict_print(method2_x,method2_labels,df_test['text'],test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Best Annotators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 1 Annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[481   0]\n",
      " [518   1]]\n",
      "Accuracy: \n",
      "0.482\n"
     ]
    }
   ],
   "source": [
    "model_predict_print(df_annotators['text'],method3_labels_top1,df_test['text'],test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 2 Annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[351 130]\n",
      " [122 397]]\n",
      "Accuracy: \n",
      "0.748\n"
     ]
    }
   ],
   "source": [
    "model_predict_print(df_annotators['text'],method3_labels_top2,df_test['text'],test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 3 Annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[480   1]\n",
      " [518   1]]\n",
      "Accuracy: \n",
      "0.481\n"
     ]
    }
   ],
   "source": [
    "model_predict_print(df_annotators['text'],method3_labels_top3,df_test['text'],test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 4 Annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[345 136]\n",
      " [118 401]]\n",
      "Accuracy: \n",
      "0.746\n"
     ]
    }
   ],
   "source": [
    "model_predict_print(df_annotators['text'],method3_labels_top4,df_test['text'],test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 5 Annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[440  41]\n",
      " [248 271]]\n",
      "Accuracy: \n",
      "0.711\n"
     ]
    }
   ],
   "source": [
    "model_predict_print(df_annotators['text'],method3_labels_top5,df_test['text'],test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 6 Annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[366 115]\n",
      " [129 390]]\n",
      "Accuracy: \n",
      "0.756\n"
     ]
    }
   ],
   "source": [
    "model_predict_print(df_annotators['text'],method3_labels_top6,df_test['text'],test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 7 Annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[434  47]\n",
      " [230 289]]\n",
      "Accuracy: \n",
      "0.723\n"
     ]
    }
   ],
   "source": [
    "model_predict_print(df_annotators['text'],method3_labels_top7,df_test['text'],test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 8 Annotators (All Annotators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[360 121]\n",
      " [126 393]]\n",
      "Accuracy: \n",
      "0.753\n"
     ]
    }
   ],
   "source": [
    "model_predict_print(df_annotators['text'],method3_labels_top8,df_test['text'],test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1 (Majority Wins) achieved the highest accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  | Method 1 | Method 2  | Method 3-1  |  Method 3-2 | Method 3-3  | Method 3-4  |\n",
    "|---|---|---|---|---|---|---|\n",
    "| Accuracy  | 0.753  |  0.699 |  0.482 |  0.748 | 0.481  | 0.746  |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
